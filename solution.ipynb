{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "2024-04-26 17:36:41 INFO: Loading these models for language: multilingual ():\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| langid    | ud      |\n",
      "=======================\n",
      "\n",
      "2024-04-26 17:36:41 INFO: Using device: cpu\n",
      "2024-04-26 17:36:41 INFO: Loading: langid\n",
      "2024-04-26 17:36:42 INFO: Done loading processors!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 332\u001b[0m\n\u001b[0;32m    328\u001b[0m                 model\u001b[38;5;241m.\u001b[39mprint_results(Dataset(filtered_embeddings, filtered_dataframe[filtered_dataframe[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my3\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m y3_class]))\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 289\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    287\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m remove_duplicates(dataframe)\n\u001b[0;32m    288\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m remove_noise(dataframe)\n\u001b[1;32m--> 289\u001b[0m dataframe[Configurations\u001b[38;5;241m.\u001b[39mTICKET_SUMMARY_COL] \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_text_to_english\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mConfigurations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTICKET_SUMMARY_COL\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    290\u001b[0m dataframe[Configurations\u001b[38;5;241m.\u001b[39mINTERACTION_CONTENT_COL] \u001b[38;5;241m=\u001b[39m translate_text_to_english(dataframe[Configurations\u001b[38;5;241m.\u001b[39mINTERACTION_CONTENT_COL]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m    292\u001b[0m grouped_dataframe \u001b[38;5;241m=\u001b[39m dataframe\u001b[38;5;241m.\u001b[39mgroupby(Configurations\u001b[38;5;241m.\u001b[39mGROUPED_COL)\n",
      "Cell \u001b[1;32mIn[2], line 200\u001b[0m, in \u001b[0;36mtranslate_text_to_english\u001b[1;34m(text_list)\u001b[0m\n\u001b[0;32m    198\u001b[0m translation_tokenizer\u001b[38;5;241m.\u001b[39msrc_lang \u001b[38;5;241m=\u001b[39m lang\n\u001b[0;32m    199\u001b[0m encoded_text \u001b[38;5;241m=\u001b[39m translation_tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 200\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtranslation_model_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced_bos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtranslation_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_lang_id\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m translated_text \u001b[38;5;241m=\u001b[39m translation_tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_tokens, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    202\u001b[0m translated_text_list\u001b[38;5;241m.\u001b[39mappend(translated_text[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1609\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1602\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1603\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1604\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   1605\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1607\u001b[0m     )\n\u001b[0;32m   1608\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[1;32m-> 1609\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1619\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1620\u001b[0m \u001b[43m        \u001b[49m\u001b[43msequential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1621\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1622\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE:\n\u001b[0;32m   1625\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1626\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:3064\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, sequential, **model_kwargs)\u001b[0m\n\u001b[0;32m   3061\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch)\n\u001b[0;32m   3063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[1;32m-> 3064\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3065\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3067\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3068\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   3072\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:1292\u001b[0m, in \u001b[0;36mM2M100ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1271\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1272\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1273\u001b[0m         )\n\u001b[0;32m   1275\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m   1276\u001b[0m     input_ids,\n\u001b[0;32m   1277\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1290\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1291\u001b[0m )\n\u001b[1;32m-> 1292\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1294\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;66;03m# move labels to the correct device to enable PP\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn\n",
    "pip install stanza\n",
    "pip install transformers\n",
    "pip install sentencepiece\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import stanza\n",
    "from stanza.pipeline.core import DownloadMethod\n",
    "from transformers import pipeline, M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "\n",
    "class Configurations:\n",
    "    TICKET_SUMMARY_COL = 'Ticket Summary'\n",
    "    INTERACTION_CONTENT_COL = 'Interaction content'\n",
    "    TYPE_COLUMNS = ['y2', 'y3', 'y4']\n",
    "    CLASS_COLUMN = 'y2'\n",
    "    GROUPED_COL = 'y1'\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    dataframe1 = pd.read_csv(\"C:/Users/user/Desktop/Engineering and Evaluating Artificial Intelligence/AppGallery.csv\", skipinitialspace=True)\n",
    "    dataframe1.rename(columns={'Type 1': 'y1', 'Type 2': 'y2', 'Type 3': 'y3', 'Type 4': 'y4'}, inplace=True)\n",
    "    dataframe2 = pd.read_csv(\"C:/Users/user/Desktop/Engineering and Evaluating Artificial Intelligence/Purchasing.csv\", skipinitialspace=True)\n",
    "    dataframe2.rename(columns={'Type 1': 'y1', 'Type 2': 'y2', 'Type 3': 'y3', 'Type 4': 'y4'}, inplace=True)\n",
    "    merged_dataframe = pd.concat([dataframe1, dataframe2])\n",
    "    merged_dataframe[Configurations.INTERACTION_CONTENT_COL] = merged_dataframe[Configurations.INTERACTION_CONTENT_COL].values.astype('U')\n",
    "    merged_dataframe[Configurations.TICKET_SUMMARY_COL] = merged_dataframe[Configurations.TICKET_SUMMARY_COL].values.astype('U')\n",
    "    merged_dataframe[\"y\"] = merged_dataframe[Configurations.CLASS_COLUMN]\n",
    "    merged_dataframe = merged_dataframe.loc[(merged_dataframe[\"y\"] != '') & (~merged_dataframe[\"y\"].isna())]\n",
    "    return merged_dataframe\n",
    "\n",
    "\n",
    "def remove_duplicates(dataset: pd.DataFrame):\n",
    "    dataset[\"deduplicated_content\"] = \"\"\n",
    "\n",
    "    customer_support_template = {\n",
    "        \"english\": [\n",
    "            \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) Customer Support team\\,?\",\n",
    "            \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE is a company incorporated under the laws of Ireland with its headquarters in Dublin, Ireland\\.?\",\n",
    "            \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE is the provider of Huawei Mobile Services to Huawei and Honor device owners in (?:Europe|\\*\\*\\*\\*\\*\\(LOC\\)), Canada, Australia, New Zealand and other countries\\.?\"\n",
    "        ],\n",
    "        \"german\": [\n",
    "            \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) Kundenservice\\,?\",\n",
    "            \"Die (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE ist eine Gesellschaft nach irischem Recht mit Sitz in Dublin, Irland\\.?\",\n",
    "            \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE ist der Anbieter von Huawei Mobile Services für Huawei- und Honor-Gerätebesitzer in Europa, Kanada, Australien, Neuseeland und anderen Ländern\\.?\"\n",
    "        ],\n",
    "        \"french\": [\n",
    "            \"L'équipe d'assistance à la clientèle d'Aspiegel\\,?\",\n",
    "            \"Die (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE est une société de droit irlandais dont le siège est à Dublin, en Irlande\\.?\",\n",
    "            \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE est le fournisseur de services mobiles Huawei aux propriétaires d'appareils Huawei et Honor en Europe, au Canada, en Australie, en Nouvelle-Zélande et dans d'autres pays\\.?\"\n",
    "        ],\n",
    "        \"spanish\": [\n",
    "            \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) Soporte Servicio al Cliente\\,?\",\n",
    "            \"Die (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) es una sociedad constituida en virtud de la legislación de Irlanda con su sede en Dublín, Irlanda\\.?\",\n",
    "            \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE es el proveedor de servicios móviles de Huawei a los propietarios de dispositivos de Huawei y Honor en Europa, Canadá, Australia, Nueva Zelanda y otros países\\.?\"\n",
    "        ],\n",
    "        \"italian\": [\n",
    "            \"Il tuo team ad (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)),?\",\n",
    "            \"Die (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE è una società costituita secondo le leggi irlandesi con sede a Dublino, Irlanda\\.?\",\n",
    "            \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE è il fornitore di servizi mobili Huawei per i proprietari di dispositivi Huawei e Honor in Europa, Canada, Australia, Nuova Zelanda e altri paesi\\.?\"\n",
    "        ],\n",
    "        \"portuguese\": [\n",
    "            \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) Customer Support team,?\",\n",
    "            \"Die (?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE é uma empresa constituída segundo as leis da Irlanda, com sede em Dublin, Irlanda\\.?\",\n",
    "            \"(?:Aspiegel|\\*\\*\\*\\*\\*\\(PERSON\\)) SE é o provedor de Huawei Mobile Services para Huawei e Honor proprietários de dispositivos na Europa, Canadá, Austrália, Nova Zelândia e outros países\\.?\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    customer_support_pattern = \"|\".join(sum(list(customer_support_template.values()), []))\n",
    "\n",
    "    email_pattern = \"(From\\s?:\\s?xxxxx@xxxx.com Sent\\s?:.{30,70}Subject\\s?:)\"\n",
    "    wrote_pattern = \"(On.{30,60}wrote:)\"\n",
    "    reply_pattern = \"(Re\\s?:|RE\\s?:)\"\n",
    "    support_issue_pattern = \"(\\*\\*\\*\\*\\*\\(PERSON\\) Support issue submit)\"\n",
    "    phone_pattern = \"(\\s?\\*\\*\\*\\*\\*\\(PHONE\\))*$\"\n",
    "\n",
    "    splitting_pattern = f\"{email_pattern}|{wrote_pattern}|{reply_pattern}|{support_issue_pattern}|{phone_pattern}\"\n",
    "\n",
    "    ticket_counts = dataset[\"Ticket id\"].value_counts()\n",
    "\n",
    "    for ticket_id in ticket_counts.index:\n",
    "        ticket_dataframe = dataset.loc[dataset['Ticket id'] == ticket_id]\n",
    "\n",
    "        unique_content_set = set()\n",
    "        deduplicated_content = []\n",
    "        for content in ticket_dataframe[Configurations.INTERACTION_CONTENT_COL]:\n",
    "            content_segments = re.split(splitting_pattern, content)\n",
    "            content_segments = [segment for segment in content_segments if segment is not None]\n",
    "            content_segments = [re.sub(splitting_pattern, \"\", segment.strip()) for segment in content_segments]\n",
    "            content_segments = [re.sub(customer_support_pattern, \"\", segment.strip()) for segment in content_segments]\n",
    "\n",
    "            current_content = []\n",
    "            for segment in content_segments:\n",
    "                if segment and segment not in unique_content_set:\n",
    "                    unique_content_set.add(segment)\n",
    "                    current_content.append(segment + \"\\n\")\n",
    "\n",
    "            deduplicated_content.append(' '.join(current_content))\n",
    "        dataset.loc[dataset[\"Ticket id\"] == ticket_id, \"deduplicated_content\"] = deduplicated_content\n",
    "\n",
    "    dataset[Configurations.INTERACTION_CONTENT_COL] = dataset['deduplicated_content']\n",
    "    dataset = dataset.drop(columns=['deduplicated_content'])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def remove_noise(dataframe: pd.DataFrame):\n",
    "    noise_pattern = \"(sv\\s*:)|(wg\\s*:)|(ynt\\s*:)|(fw(d)?\\s*:)|(r\\s*:)|(re\\s*:)|([|])|(aspiegel support issue submit)|(null)|(nan)|((bonus place my )?support.pt 自动回复:)\"\n",
    "    dataframe[Configurations.TICKET_SUMMARY_COL] = (\n",
    "        dataframe[Configurations.TICKET_SUMMARY_COL]\n",
    "        .str.lower()\n",
    "        .replace(noise_pattern, \" \", regex=True)\n",
    "        .replace(r'\\s+', ' ', regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "    dataframe[Configurations.INTERACTION_CONTENT_COL] = dataframe[Configurations.INTERACTION_CONTENT_COL].str.lower()\n",
    "\n",
    "    noise_patterns = [\n",
    "    \"(from :)|(subject :)|(sent :)|(r\\\\s*:)|(re\\\\s*:)\",\n",
    "    \"(january|february|march|april|may|june|july|august|september|october|november|december)\",\n",
    "    \"(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\",\n",
    "    \"(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\",\n",
    "    \"\\\\d{2}(:|.)\\\\d{2}\",\n",
    "    \"(xxxxx@xxxx\\\\.com)|(\\\\*{5}([a-z]+))\",\n",
    "    \"dear ((customer)|(user))\",\n",
    "    \"dear\",\n",
    "    \"(hello)|(hallo)|(hi )|(hi there)\",\n",
    "    \"good morning\",\n",
    "    \"thank you for your patience ((during (our)? investigation)|(and cooperation))?\",\n",
    "    \"thank you for contacting us\",\n",
    "    \"thank you for your availability\",\n",
    "    \"thank you for providing us this information\",\n",
    "    \"thank you for contacting\",\n",
    "    \"thank you for reaching us (back)?\",\n",
    "    \"thank you for patience\",\n",
    "    \"thank you for (your)? reply\",\n",
    "    \"thank you for (your)? response\",\n",
    "    \"thank you for (your)? cooperation\",\n",
    "    \"thank you for providing us with more information\",\n",
    "    \"thank you very kindly\",\n",
    "    \"thank you( very much)?\",\n",
    "    \"i would like to follow up on the case you raised on the date\",\n",
    "    \"i will do my very best to assist you\",\n",
    "    \"in order to give you the best solution\",\n",
    "    \"could you please clarify your request with following information:\",\n",
    "    \"in this matter\",\n",
    "    \"we hope you(( are)|('re)) doing ((fine)|(well))\",\n",
    "    \"i would like to follow up on the case you raised on\",\n",
    "    \"we apologize for the inconvenience\",\n",
    "    \"sent from my huawei (cell )?phone\",\n",
    "    \"original message\",\n",
    "    \"customer support team\",\n",
    "    \"(aspiegel )?se is a company incorporated under the laws of ireland with its headquarters in dublin, ireland.\",\n",
    "    \"(aspiegel )?se is the provider of huawei mobile services to huawei and honor device owners in\",\n",
    "    \"canada, australia, new zealand and other countries\",\n",
    "    \"\\\\d+\",\n",
    "    \"[^0-9a-zA-Z]+\",\n",
    "    \"(\\\\s|^).{1}(\\\\s|$)\"\n",
    "]\n",
    "\n",
    "    for noise_pattern in noise_patterns:\n",
    "        dataframe[Configurations.INTERACTION_CONTENT_COL] = dataframe[Configurations.INTERACTION_CONTENT_COL].replace(noise_pattern, \" \", regex=True)\n",
    "    dataframe[Configurations.INTERACTION_CONTENT_COL] = dataframe[Configurations.INTERACTION_CONTENT_COL].replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "    frequent_y1 = dataframe.y1.value_counts()[dataframe.y1.value_counts() > 10].index\n",
    "    dataframe = dataframe.loc[dataframe.y1.isin(frequent_y1)]\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def translate_text_to_english(text_list: list[str]):\n",
    "    translation_model = \"facebook/m2m100_418M\"\n",
    "    translation_pipeline = pipeline(task='text2text-generation', model=translation_model)\n",
    "    translation_model_instance = M2M100ForConditionalGeneration.from_pretrained(translation_model)\n",
    "    translation_tokenizer = M2M100Tokenizer.from_pretrained(translation_model)\n",
    "\n",
    "    language_detector = stanza.Pipeline(lang=\"multilingual\", processors=\"langid\", download_method=DownloadMethod.REUSE_RESOURCES)\n",
    "\n",
    "    translated_text_list = []\n",
    "    for text in text_list:\n",
    "        if not text:\n",
    "            translated_text_list.append(text)\n",
    "            continue\n",
    "\n",
    "        detected_lang = language_detector(text)\n",
    "        if detected_lang.lang == \"en\":\n",
    "            translated_text_list.append(text)\n",
    "        else:\n",
    "            lang = detected_lang.lang\n",
    "            if lang == \"fro\":\n",
    "                lang = \"fr\"\n",
    "            elif lang == \"la\":\n",
    "                lang = \"it\"\n",
    "            elif lang == \"nn\":\n",
    "                lang = \"no\"\n",
    "            elif lang == \"kmr\":\n",
    "                lang = \"tr\"\n",
    "\n",
    "            translation_tokenizer.src_lang = lang\n",
    "            encoded_text = translation_tokenizer(text, return_tensors=\"pt\")\n",
    "            generated_tokens = translation_model_instance.generate(**encoded_text, forced_bos_token_id=translation_tokenizer.get_lang_id(\"en\"))\n",
    "            translated_text = translation_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            translated_text_list.append(translated_text[0])\n",
    "    return translated_text_list\n",
    "\n",
    "\n",
    "def generate_tfidf_embeddings(dataframe: pd.DataFrame):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=2000, min_df=4, max_df=0.90)\n",
    "    text_data = dataframe[Configurations.TICKET_SUMMARY_COL] + ' ' + dataframe[Configurations.INTERACTION_CONTENT_COL]\n",
    "    embeddings = tfidf_vectorizer.fit_transform(text_data).toarray()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, embeddings: np.ndarray, dataframe: pd.DataFrame):\n",
    "        labels = dataframe[\"y\"].to_numpy()\n",
    "        label_series = pd.Series(labels)\n",
    "        frequent_labels = label_series.value_counts()[label_series.value_counts() >= 3].index\n",
    "\n",
    "        if len(frequent_labels) < 1:\n",
    "            print(\"None of the classes have more than 3 records: Skipping ...\")\n",
    "            self.train_embeddings = None\n",
    "            return\n",
    "\n",
    "        filtered_labels = labels[label_series.isin(frequent_labels)]\n",
    "        filtered_embeddings = embeddings[label_series.isin(frequent_labels)]\n",
    "        new_test_size = embeddings.shape[0] * 0.2 / filtered_embeddings.shape[0]\n",
    "        (\n",
    "            self.train_embeddings,\n",
    "            self.test_embeddings,\n",
    "            self.train_labels,\n",
    "            self.test_labels\n",
    "        ) = train_test_split(filtered_embeddings, filtered_labels, test_size=new_test_size, random_state=0, stratify=filtered_labels)\n",
    "        self.labels = filtered_labels\n",
    "        self.unique_classes = frequent_labels\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "\n",
    "    def get_train_embeddings(self):\n",
    "        return self.train_embeddings\n",
    "\n",
    "    def get_test_embeddings(self):\n",
    "        return self.test_embeddings\n",
    "\n",
    "    def get_train_labels(self):\n",
    "        return self.train_labels\n",
    "\n",
    "    def get_test_labels(self):\n",
    "        return self.test_labels\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        return self.embeddings\n",
    "\n",
    "\n",
    "class RandomForestModel:\n",
    "    def __init__(self, model_name, embeddings):\n",
    "        self.model_name = model_name\n",
    "        self.embeddings = embeddings\n",
    "        self.model = RandomForestClassifier(n_estimators=1000, random_state=0, class_weight='balanced_subsample')\n",
    "        self.predictions = None\n",
    "\n",
    "    def train(self, dataset):\n",
    "        train_embeddings, train_labels = dataset.get_train_embeddings(), dataset.get_train_labels()\n",
    "        self.model.fit(train_embeddings, train_labels)\n",
    "\n",
    "    def predict(self, test_embeddings: np.ndarray):\n",
    "        self.predictions = self.model.predict(test_embeddings)\n",
    "\n",
    "    def print_results(self, dataset):\n",
    "        print(classification_report(dataset.get_test_labels(), self.predictions))\n",
    "\n",
    "    def data_transform(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def model_training_and_prediction(dataset, dataframe, model_name):\n",
    "    print(\"RandomForest\")\n",
    "    model = RandomForestModel(\"RandomForest\", dataset.get_embeddings())\n",
    "    model.train(dataset)\n",
    "    model.predict(dataset.get_test_embeddings())\n",
    "    model.print_results(dataset)\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataframe = load_dataset()\n",
    "    dataframe = remove_duplicates(dataframe)\n",
    "    dataframe = remove_noise(dataframe)\n",
    "    dataframe[Configurations.TICKET_SUMMARY_COL] = translate_text_to_english(dataframe[Configurations.TICKET_SUMMARY_COL].tolist())\n",
    "    dataframe[Configurations.INTERACTION_CONTENT_COL] = translate_text_to_english(dataframe[Configurations.INTERACTION_CONTENT_COL].tolist())\n",
    "\n",
    "    grouped_dataframe = dataframe.groupby(Configurations.GROUPED_COL)\n",
    "    for group_name, group_dataframe in grouped_dataframe:\n",
    "        print(group_name)\n",
    "        embeddings = generate_tfidf_embeddings(group_dataframe)\n",
    "        dataset = Dataset(embeddings, group_dataframe)\n",
    "\n",
    "        if dataset.train_embeddings is None:\n",
    "            continue\n",
    "\n",
    "        # Chained Multi-outputs\n",
    "        print(\"Chained Multi-outputs:\")\n",
    "        for combination in [[\"y2\"], [\"y2\", \"y3\"], [\"y2\", \"y3\", \"y4\"]]:\n",
    "            print(f\"Combination: {combination}\")\n",
    "            model = RandomForestModel(\"RandomForest\", dataset.get_embeddings())\n",
    "            model.train(Dataset(embeddings, group_dataframe[combination]))\n",
    "            model.predict(dataset.get_test_embeddings())\n",
    "            model.print_results(Dataset(embeddings, group_dataframe[combination]))\n",
    "\n",
    "        # Hierarchical Modeling\n",
    "        print(\"Hierarchical Modeling:\")\n",
    "        y2_classes = group_dataframe[\"y2\"].unique()\n",
    "        for y2_class in y2_classes:\n",
    "            print(f\"y2 Class: {y2_class}\")\n",
    "            filtered_dataframe = group_dataframe[group_dataframe[\"y2\"] == y2_class]\n",
    "            filtered_embeddings = generate_tfidf_embeddings(filtered_dataframe)\n",
    "            filtered_dataset = Dataset(filtered_embeddings, filtered_dataframe)\n",
    "\n",
    "            if filtered_dataset.train_embeddings is None:\n",
    "                continue\n",
    "\n",
    "            y3_classes = filtered_dataframe[\"y3\"].unique()\n",
    "            for y3_class in y3_classes:\n",
    "                print(f\"y3 Class: {y3_class}\")\n",
    "                model = RandomForestModel(\"RandomForest\", filtered_dataset.get_embeddings())\n",
    "                model.train(Dataset(filtered_embeddings, filtered_dataframe[filtered_dataframe[\"y3\"] == y3_class]))\n",
    "                model.predict(filtered_dataset.get_test_embeddings())\n",
    "                model.print_results(Dataset(filtered_embeddings, filtered_dataframe[filtered_dataframe[\"y3\"] == y3_class]))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
